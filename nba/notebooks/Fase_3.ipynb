{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c49cc9a",
   "metadata": {},
   "source": [
    "# üîß Fase 3: Preparaci√≥n de los Datos NBA\n",
    "\n",
    "## üìã Objetivo\n",
    "Transformar los datos de la NBA para que est√©n listos para el modelado de machine learning, aplicando t√©cnicas de limpieza, transformaci√≥n y divisi√≥n basadas en los insights obtenidos en las fases anteriores.\n",
    "\n",
    "## üéØ Metodolog√≠a\n",
    "Esta fase se enfoca en:\n",
    "- **Limpieza sistem√°tica** de datos basada en el an√°lisis de la Fase 2\n",
    "- **Transformaciones inteligentes** que preserven la informaci√≥n relevante\n",
    "- **Divisi√≥n estrat√©gica** del dataset para validaci√≥n robusta\n",
    "- **Justificaci√≥n t√©cnica** de cada decisi√≥n tomada\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Contenido del An√°lisis\n",
    "1. **Carga y Revisi√≥n** - Recuperar insights de fases anteriores\n",
    "2. **Limpieza de Datos** - Imputaci√≥n, outliers, inconsistencias\n",
    "3. **Transformaciones** - Codificaci√≥n, normalizaci√≥n, fechas\n",
    "4. **Divisi√≥n Estrat√©gica** - Train/test split con validaci√≥n\n",
    "5. **Justificaci√≥n T√©cnica** - Fundamentos estad√≠sticos y matem√°ticos\n",
    "6. **Dataset Final** - Verificaci√≥n y documentaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamentos Te√≥ricos\n",
    "- **Estad√≠stica Descriptiva**: Medidas de tendencia central y dispersi√≥n\n",
    "- **√Ålgebra Lineal**: Transformaciones matriciales y escalado\n",
    "- **Teor√≠a de Probabilidad**: Distribuciones y muestreo\n",
    "- **Machine Learning**: Preprocesamiento y validaci√≥n cruzada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da60772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ CARGANDO DATOS Y REVISANDO INSIGHTS\n",
      "==================================================\n",
      "üìä Cargando dataset principal...\n",
      "‚úÖ Dataset cargado: (65698, 55)\n",
      "‚úÖ Estad√≠sticas adicionales cargadas: (28271, 26)\n",
      "\n",
      "üîç REVISI√ìN DE INSIGHTS DE FASES ANTERIORES:\n",
      "‚Ä¢ Total de partidos: 65,698\n",
      "‚Ä¢ Variables disponibles: 55\n",
      "‚Ä¢ Rango temporal: 1946-11-01 00:00:00 a 2023-06-12 00:00:00\n",
      "‚Ä¢ Temporadas: 225\n",
      "‚Ä¢ Porcentaje de victorias locales: 61.9%\n",
      "‚Ä¢ Variables num√©ricas: 45\n",
      "‚Ä¢ Variables categ√≥ricas: 10\n",
      "\n",
      "‚úÖ Carga y revisi√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# üì• Carga de Datos y Revisi√≥n de Insights\n",
    "print(\"üîÑ CARGANDO DATOS Y REVISANDO INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Cargar dataset principal\n",
    "print(\"üìä Cargando dataset principal...\")\n",
    "games_df = pd.read_csv('../data/01_raw/game.csv')\n",
    "print(f\"‚úÖ Dataset cargado: {games_df.shape}\")\n",
    "\n",
    "# Cargar estad√≠sticas adicionales para enriquecimiento\n",
    "other_stats_df = pd.read_csv('../data/01_raw/other_stats.csv')\n",
    "print(f\"‚úÖ Estad√≠sticas adicionales cargadas: {other_stats_df.shape}\")\n",
    "\n",
    "# Revisar insights de fases anteriores\n",
    "print(f\"\\nüîç REVISI√ìN DE INSIGHTS DE FASES ANTERIORES:\")\n",
    "print(f\"‚Ä¢ Total de partidos: {len(games_df):,}\")\n",
    "print(f\"‚Ä¢ Variables disponibles: {len(games_df.columns)}\")\n",
    "print(f\"‚Ä¢ Rango temporal: {games_df['game_date'].min()} a {games_df['game_date'].max()}\")\n",
    "print(f\"‚Ä¢ Temporadas: {games_df['season_id'].nunique()}\")\n",
    "\n",
    "# Verificar variable objetivo\n",
    "win_percentage = (games_df['wl_home'] == 'W').mean()\n",
    "print(f\"‚Ä¢ Porcentaje de victorias locales: {win_percentage:.1%}\")\n",
    "\n",
    "# Identificar tipos de variables\n",
    "numeric_cols = games_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = games_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"‚Ä¢ Variables num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"‚Ä¢ Variables categ√≥ricas: {len(categorical_cols)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Carga y revisi√≥n completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8816658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ LIMPIEZA DE DATOS\n",
      "==============================\n",
      "üìä Dataset original: (65698, 55)\n",
      "\n",
      "‚ùå 1. AN√ÅLISIS DE VALORES NULOS\n",
      "----------------------------------------\n",
      "üìä Columnas con valores nulos: 36\n",
      "üîç Top 10 columnas con m√°s valores nulos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valores_Nulos</th>\n",
       "      <th>Porcentaje_Nulos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fg3_pct_home</th>\n",
       "      <td>19074</td>\n",
       "      <td>29.032847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreb_home</th>\n",
       "      <td>18999</td>\n",
       "      <td>28.918689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreb_away</th>\n",
       "      <td>18998</td>\n",
       "      <td>28.917166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fg3_pct_away</th>\n",
       "      <td>18962</td>\n",
       "      <td>28.862370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oreb_away</th>\n",
       "      <td>18936</td>\n",
       "      <td>28.822795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oreb_home</th>\n",
       "      <td>18936</td>\n",
       "      <td>28.822795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stl_home</th>\n",
       "      <td>18849</td>\n",
       "      <td>28.690371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stl_away</th>\n",
       "      <td>18849</td>\n",
       "      <td>28.690371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tov_away</th>\n",
       "      <td>18685</td>\n",
       "      <td>28.440744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tov_home</th>\n",
       "      <td>18684</td>\n",
       "      <td>28.439222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "              Valores_Nulos  Porcentaje_Nulos\n",
       "fg3_pct_home          \u001b[1;36m19074\u001b[0m         \u001b[1;36m29.032847\u001b[0m\n",
       "dreb_home             \u001b[1;36m18999\u001b[0m         \u001b[1;36m28.918689\u001b[0m\n",
       "dreb_away             \u001b[1;36m18998\u001b[0m         \u001b[1;36m28.917166\u001b[0m\n",
       "fg3_pct_away          \u001b[1;36m18962\u001b[0m         \u001b[1;36m28.862370\u001b[0m\n",
       "oreb_away             \u001b[1;36m18936\u001b[0m         \u001b[1;36m28.822795\u001b[0m\n",
       "oreb_home             \u001b[1;36m18936\u001b[0m         \u001b[1;36m28.822795\u001b[0m\n",
       "stl_home              \u001b[1;36m18849\u001b[0m         \u001b[1;36m28.690371\u001b[0m\n",
       "stl_away              \u001b[1;36m18849\u001b[0m         \u001b[1;36m28.690371\u001b[0m\n",
       "tov_away              \u001b[1;36m18685\u001b[0m         \u001b[1;36m28.440744\u001b[0m\n",
       "tov_home              \u001b[1;36m18684\u001b[0m         \u001b[1;36m28.439222\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ ESTRATEGIA DE IMPUTACI√ìN:\n",
      "‚Ä¢ Variables num√©ricas (34): Imputaci√≥n con mediana\n",
      "  - fg3_pct_home: 0 ‚Üí 0 (mediana: 0.35)\n",
      "  - dreb_home: 0 ‚Üí 0 (mediana: 31.00)\n",
      "  - dreb_away: 0 ‚Üí 0 (mediana: 30.00)\n",
      "  - fg3_pct_away: 0 ‚Üí 0 (mediana: 0.33)\n",
      "  - oreb_away: 0 ‚Üí 0 (mediana: 11.00)\n",
      "  - oreb_home: 0 ‚Üí 0 (mediana: 12.00)\n",
      "  - stl_home: 0 ‚Üí 0 (mediana: 8.00)\n",
      "  - stl_away: 0 ‚Üí 0 (mediana: 8.00)\n",
      "  - tov_away: 0 ‚Üí 0 (mediana: 15.00)\n",
      "  - tov_home: 0 ‚Üí 0 (mediana: 15.00)\n",
      "  - fg3a_away: 0 ‚Üí 0 (mediana: 16.00)\n",
      "  - fg3a_home: 0 ‚Üí 0 (mediana: 16.00)\n",
      "  - blk_home: 0 ‚Üí 0 (mediana: 5.00)\n",
      "  - blk_away: 0 ‚Üí 0 (mediana: 4.00)\n",
      "  - ast_home: 0 ‚Üí 0 (mediana: 24.00)\n",
      "  - ast_away: 0 ‚Üí 0 (mediana: 22.00)\n",
      "  - reb_home: 0 ‚Üí 0 (mediana: 43.00)\n",
      "  - reb_away: 0 ‚Üí 0 (mediana: 42.00)\n",
      "  - fg_pct_home: 0 ‚Üí 0 (mediana: 0.47)\n",
      "  - fg_pct_away: 0 ‚Üí 0 (mediana: 0.46)\n",
      "  - fga_home: 0 ‚Üí 0 (mediana: 84.00)\n",
      "  - fga_away: 0 ‚Üí 0 (mediana: 83.00)\n",
      "  - fg3m_home: 0 ‚Üí 0 (mediana: 5.00)\n",
      "  - fg3m_away: 0 ‚Üí 0 (mediana: 5.00)\n",
      "  - ft_pct_home: 0 ‚Üí 0 (mediana: 0.76)\n",
      "  - ft_pct_away: 0 ‚Üí 0 (mediana: 0.76)\n",
      "  - fta_home: 0 ‚Üí 0 (mediana: 27.00)\n",
      "  - fta_away: 0 ‚Üí 0 (mediana: 25.00)\n",
      "  - pf_home: 0 ‚Üí 0 (mediana: 22.00)\n",
      "  - pf_away: 0 ‚Üí 0 (mediana: 23.00)\n",
      "  - ftm_home: 0 ‚Üí 0 (mediana: 20.00)\n",
      "  - fgm_away: 0 ‚Üí 0 (mediana: 38.00)\n",
      "  - fgm_home: 0 ‚Üí 0 (mediana: 40.00)\n",
      "  - ftm_away: 0 ‚Üí 0 (mediana: 19.00)\n",
      "‚Ä¢ Variables categ√≥ricas (2): Imputaci√≥n con moda\n",
      "  - wl_home: 0 ‚Üí 0 (moda: W)\n",
      "  - wl_away: 0 ‚Üí 0 (moda: L)\n",
      "\n",
      "‚úÖ Valores nulos restantes: 0\n",
      "\n",
      "üîç 2. DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
      "--------------------------------------------------\n",
      "üìä An√°lisis de outliers (M√©todo IQR):\n",
      "  ‚Ä¢ pts_home: 849 outliers capados (1.29%)\n",
      "  ‚Ä¢ pts_away: 943 outliers capados (1.44%)\n",
      "  ‚Ä¢ fg_pct_home: 3290 outliers capados (5.01%)\n",
      "  ‚Ä¢ fg_pct_away: 3775 outliers capados (5.75%)\n",
      "  ‚Ä¢ reb_home: 2282 outliers capados (3.47%)\n",
      "  ‚Ä¢ reb_away: 3286 outliers capados (5.00%)\n",
      "  ‚Ä¢ ast_home: 3826 outliers capados (5.82%)\n",
      "  ‚Ä¢ ast_away: 5510 outliers capados (8.39%)\n",
      "\n",
      "‚úÖ Total de outliers tratados: 23761\n",
      "\n",
      "üîß 3. CORRECCI√ìN DE INCONSISTENCIAS\n",
      "----------------------------------------\n",
      "‚úÖ Fechas convertidas a datetime\n",
      "  ‚Ä¢ ft_pct_home: Convertido de escala 0-100 a 0-1\n",
      "  ‚Ä¢ ft_pct_away: Convertido de escala 0-100 a 0-1\n",
      "\n",
      "üìä Dataset despu√©s de limpieza: (65698, 55)\n",
      "‚úÖ Limpieza de datos completada\n"
     ]
    }
   ],
   "source": [
    "# üßπ Limpieza de Datos\n",
    "print(\"üßπ LIMPIEZA DE DATOS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Crear copia del dataset para trabajar\n",
    "df_clean = games_df.copy()\n",
    "print(f\"üìä Dataset original: {df_clean.shape}\")\n",
    "\n",
    "# 1. AN√ÅLISIS DE VALORES NULOS\n",
    "print(f\"\\n‚ùå 1. AN√ÅLISIS DE VALORES NULOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calcular valores nulos por columna\n",
    "null_analysis = pd.DataFrame({\n",
    "    'Valores_Nulos': df_clean.isnull().sum(),\n",
    "    'Porcentaje_Nulos': (df_clean.isnull().sum() / len(df_clean)) * 100\n",
    "}).sort_values('Porcentaje_Nulos', ascending=False)\n",
    "\n",
    "# Filtrar columnas con valores nulos\n",
    "null_columns = null_analysis[null_analysis['Valores_Nulos'] > 0]\n",
    "print(f\"üìä Columnas con valores nulos: {len(null_columns)}\")\n",
    "\n",
    "if len(null_columns) > 0:\n",
    "    print(\"üîç Top 10 columnas con m√°s valores nulos:\")\n",
    "    display(null_columns.head(10))\n",
    "    \n",
    "    # Estrategia de imputaci√≥n basada en el tipo de variable\n",
    "    print(f\"\\nüéØ ESTRATEGIA DE IMPUTACI√ìN:\")\n",
    "    \n",
    "    # Para variables num√©ricas: usar mediana (robusta a outliers)\n",
    "    numeric_null_cols = [col for col in null_columns.index if col in numeric_cols]\n",
    "    if numeric_null_cols:\n",
    "        print(f\"‚Ä¢ Variables num√©ricas ({len(numeric_null_cols)}): Imputaci√≥n con mediana\")\n",
    "        for col in numeric_null_cols:\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"  - {col}: {df_clean[col].isnull().sum()} ‚Üí 0 (mediana: {median_value:.2f})\")\n",
    "    \n",
    "    # Para variables categ√≥ricas: usar moda\n",
    "    categorical_null_cols = [col for col in null_columns.index if col in categorical_cols]\n",
    "    if categorical_null_cols:\n",
    "        print(f\"‚Ä¢ Variables categ√≥ricas ({len(categorical_null_cols)}): Imputaci√≥n con moda\")\n",
    "        for col in categorical_null_cols:\n",
    "            mode_value = df_clean[col].mode().iloc[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: {df_clean[col].isnull().sum()} ‚Üí 0 (moda: {mode_value})\")\n",
    "\n",
    "# Verificar que no queden valores nulos\n",
    "remaining_nulls = df_clean.isnull().sum().sum()\n",
    "print(f\"\\n‚úÖ Valores nulos restantes: {remaining_nulls}\")\n",
    "\n",
    "# 2. DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
    "print(f\"\\nüîç 2. DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detectar outliers usando el m√©todo IQR (Rango Intercuart√≠lico)\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Variables clave para an√°lisis de outliers\n",
    "outlier_vars = ['pts_home', 'pts_away', 'fg_pct_home', 'fg_pct_away', \n",
    "                'reb_home', 'reb_away', 'ast_home', 'ast_away']\n",
    "\n",
    "outlier_summary = []\n",
    "outliers_removed = 0\n",
    "\n",
    "print(\"üìä An√°lisis de outliers (M√©todo IQR):\")\n",
    "for var in outlier_vars:\n",
    "    if var in df_clean.columns:\n",
    "        outliers, lower, upper = detect_outliers_iqr(df_clean, var)\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df_clean)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Variable': var,\n",
    "            'Outliers': outlier_count,\n",
    "            'Porcentaje': f\"{outlier_percentage:.2f}%\",\n",
    "            'L√≠mite_Inferior': f\"{lower:.2f}\",\n",
    "            'L√≠mite_Superior': f\"{upper:.2f}\"\n",
    "        })\n",
    "        \n",
    "        # Estrategia: Cap outliers en lugar de eliminarlos (preservar informaci√≥n)\n",
    "        if outlier_count > 0:\n",
    "            df_clean[var] = np.clip(df_clean[var], lower, upper)\n",
    "            outliers_removed += outlier_count\n",
    "            print(f\"  ‚Ä¢ {var}: {outlier_count} outliers capados ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total de outliers tratados: {outliers_removed}\")\n",
    "\n",
    "# 3. CORRECCI√ìN DE INCONSISTENCIAS\n",
    "print(f\"\\nüîß 3. CORRECCI√ìN DE INCONSISTENCIAS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar consistencia en fechas\n",
    "df_clean['game_date'] = pd.to_datetime(df_clean['game_date'])\n",
    "print(f\"‚úÖ Fechas convertidas a datetime\")\n",
    "\n",
    "# Verificar consistencia en porcentajes (deben estar entre 0 y 1)\n",
    "percentage_cols = [col for col in df_clean.columns if 'pct' in col.lower()]\n",
    "for col in percentage_cols:\n",
    "    if col in df_clean.columns:\n",
    "        # Convertir porcentajes que est√©n en escala 0-100 a 0-1\n",
    "        if df_clean[col].max() > 1:\n",
    "            df_clean[col] = df_clean[col] / 100\n",
    "            print(f\"  ‚Ä¢ {col}: Convertido de escala 0-100 a 0-1\")\n",
    "\n",
    "# Verificar que no haya valores negativos en estad√≠sticas que no deber√≠an tenerlos\n",
    "positive_cols = ['pts_home', 'pts_away', 'reb_home', 'reb_away', 'ast_home', 'ast_away']\n",
    "for col in positive_cols:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            df_clean[col] = np.maximum(df_clean[col], 0)\n",
    "            print(f\"  ‚Ä¢ {col}: {negative_count} valores negativos corregidos a 0\")\n",
    "\n",
    "print(f\"\\nüìä Dataset despu√©s de limpieza: {df_clean.shape}\")\n",
    "print(\"‚úÖ Limpieza de datos completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26813bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ TRANSFORMACIONES DE DATOS\n",
      "===================================\n",
      "\n",
      "üìÖ 1. CONVERSI√ìN DE FECHAS A VARIABLES √öTILES\n",
      "--------------------------------------------------\n",
      "‚úÖ Variables de fecha creadas:\n",
      "  ‚Ä¢ year, month, day, day_of_week, day_of_year, week_of_year\n",
      "  ‚Ä¢ is_weekend, is_playoff_season\n",
      "\n",
      "üè∑Ô∏è 2. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
      "--------------------------------------------------\n",
      "‚úÖ Variable objetivo binaria creada: home_win\n",
      "‚úÖ season_type codificado con One-Hot: ['season_All Star', 'season_All-Star', 'season_Playoffs', 'season_Pre Season', 'season_Regular Season']\n",
      "‚úÖ Equipos codificados con Label Encoding\n",
      "  ‚Ä¢ Equipos locales √∫nicos: 97\n",
      "  ‚Ä¢ Equipos visitantes √∫nicos: 101\n",
      "\n",
      "üßÆ 3. CREACI√ìN DE VARIABLES DERIVADAS\n",
      "----------------------------------------\n",
      "‚úÖ Variables diferenciales creadas:\n",
      "  ‚Ä¢ pts_diff, fg_pct_diff, reb_diff, ast_diff, stl_diff, blk_diff, tov_diff\n",
      "‚úÖ Variables de eficiencia creadas:\n",
      "  ‚Ä¢ home_efficiency, away_efficiency, efficiency_diff\n",
      "\n",
      "üìè 4. NORMALIZACI√ìN Y ESTANDARIZACI√ìN\n",
      "---------------------------------------------\n",
      "üìä Variables seleccionadas para escalado: 34\n",
      "‚úÖ StandardScaler aplicado:\n",
      "  ‚Ä¢ Media de variables escaladas: 0.000000\n",
      "  ‚Ä¢ Desviaci√≥n est√°ndar: 1.000008\n",
      "\n",
      "‚úÖ 5. VERIFICACI√ìN DE TRANSFORMACIONES\n",
      "----------------------------------------\n",
      "üìä Dataset despu√©s de transformaciones: (65698, 81)\n",
      "‚Ä¢ Variables originales: 55\n",
      "‚Ä¢ Variables despu√©s de transformaciones: 81\n",
      "‚Ä¢ Nuevas variables creadas: 26\n",
      "‚Ä¢ Valores infinitos: 0\n",
      "‚Ä¢ Valores nulos: 0\n",
      "‚úÖ Transformaciones completadas\n"
     ]
    }
   ],
   "source": [
    "# üîÑ Transformaciones de Datos\n",
    "print(\"üîÑ TRANSFORMACIONES DE DATOS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Crear copia para transformaciones\n",
    "df_transformed = df_clean.copy()\n",
    "\n",
    "# 1. CONVERSI√ìN DE FECHAS A VARIABLES √öTILES\n",
    "print(f\"\\nüìÖ 1. CONVERSI√ìN DE FECHAS A VARIABLES √öTILES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_transformed['year'] = df_transformed['game_date'].dt.year\n",
    "df_transformed['month'] = df_transformed['game_date'].dt.month\n",
    "df_transformed['day'] = df_transformed['game_date'].dt.day\n",
    "df_transformed['day_of_week'] = df_transformed['game_date'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "df_transformed['day_of_year'] = df_transformed['game_date'].dt.dayofyear\n",
    "df_transformed['week_of_year'] = df_transformed['game_date'].dt.isocalendar().week\n",
    "\n",
    "# Crear variables estacionales\n",
    "df_transformed['is_weekend'] = (df_transformed['day_of_week'] >= 5).astype(int)\n",
    "df_transformed['is_playoff_season'] = (df_transformed['month'].isin([4, 5, 6])).astype(int)  # Abril-Junio\n",
    "\n",
    "print(\"‚úÖ Variables de fecha creadas:\")\n",
    "print(\"  ‚Ä¢ year, month, day, day_of_week, day_of_year, week_of_year\")\n",
    "print(\"  ‚Ä¢ is_weekend, is_playoff_season\")\n",
    "\n",
    "# 2. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
    "print(f\"\\nüè∑Ô∏è 2. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Identificar variables categ√≥ricas relevantes\n",
    "categorical_features = ['wl_home', 'season_type', 'team_abbreviation_home', 'team_abbreviation_away']\n",
    "\n",
    "# Crear variable objetivo binaria\n",
    "df_transformed['home_win'] = (df_transformed['wl_home'] == 'W').astype(int)\n",
    "print(f\"‚úÖ Variable objetivo binaria creada: home_win\")\n",
    "\n",
    "# Codificar season_type (One-Hot Encoding para preservar informaci√≥n)\n",
    "season_type_dummies = pd.get_dummies(df_transformed['season_type'], prefix='season')\n",
    "df_transformed = pd.concat([df_transformed, season_type_dummies], axis=1)\n",
    "print(f\"‚úÖ season_type codificado con One-Hot: {list(season_type_dummies.columns)}\")\n",
    "\n",
    "# Codificar equipos (Label Encoding para reducir dimensionalidad)\n",
    "le_home = LabelEncoder()\n",
    "le_away = LabelEncoder()\n",
    "\n",
    "df_transformed['team_home_encoded'] = le_home.fit_transform(df_transformed['team_abbreviation_home'])\n",
    "df_transformed['team_away_encoded'] = le_away.fit_transform(df_transformed['team_abbreviation_away'])\n",
    "print(f\"‚úÖ Equipos codificados con Label Encoding\")\n",
    "print(f\"  ‚Ä¢ Equipos locales √∫nicos: {df_transformed['team_home_encoded'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Equipos visitantes √∫nicos: {df_transformed['team_away_encoded'].nunique()}\")\n",
    "\n",
    "# 3. CREACI√ìN DE VARIABLES DERIVADAS\n",
    "print(f\"\\nüßÆ 3. CREACI√ìN DE VARIABLES DERIVADAS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Diferenciales entre equipos (m√°s informativos que valores absolutos)\n",
    "df_transformed['pts_diff'] = df_transformed['pts_home'] - df_transformed['pts_away']\n",
    "df_transformed['fg_pct_diff'] = df_transformed['fg_pct_home'] - df_transformed['fg_pct_away']\n",
    "df_transformed['reb_diff'] = df_transformed['reb_home'] - df_transformed['reb_away']\n",
    "df_transformed['ast_diff'] = df_transformed['ast_home'] - df_transformed['ast_away']\n",
    "df_transformed['stl_diff'] = df_transformed['stl_home'] - df_transformed['stl_away']\n",
    "df_transformed['blk_diff'] = df_transformed['blk_home'] - df_transformed['blk_away']\n",
    "df_transformed['tov_diff'] = df_transformed['tov_home'] - df_transformed['tov_away']\n",
    "\n",
    "print(\"‚úÖ Variables diferenciales creadas:\")\n",
    "print(\"  ‚Ä¢ pts_diff, fg_pct_diff, reb_diff, ast_diff, stl_diff, blk_diff, tov_diff\")\n",
    "\n",
    "# Ratios de eficiencia\n",
    "df_transformed['home_efficiency'] = df_transformed['pts_home'] / (df_transformed['fga_home'] + 0.001)  # Evitar divisi√≥n por 0\n",
    "df_transformed['away_efficiency'] = df_transformed['pts_away'] / (df_transformed['fga_away'] + 0.001)\n",
    "df_transformed['efficiency_diff'] = df_transformed['home_efficiency'] - df_transformed['away_efficiency']\n",
    "\n",
    "print(\"‚úÖ Variables de eficiencia creadas:\")\n",
    "print(\"  ‚Ä¢ home_efficiency, away_efficiency, efficiency_diff\")\n",
    "\n",
    "# 4. NORMALIZACI√ìN Y ESTANDARIZACI√ìN\n",
    "print(f\"\\nüìè 4. NORMALIZACI√ìN Y ESTANDARIZACI√ìN\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Seleccionar variables num√©ricas para escalado\n",
    "numeric_features = [\n",
    "    'pts_home', 'pts_away', 'fg_pct_home', 'fg_pct_away', 'fg3_pct_home', 'fg3_pct_away',\n",
    "    'ft_pct_home', 'ft_pct_away', 'reb_home', 'reb_away', 'oreb_home', 'oreb_away',\n",
    "    'dreb_home', 'dreb_away', 'ast_home', 'ast_away', 'stl_home', 'stl_away',\n",
    "    'blk_home', 'blk_away', 'tov_home', 'tov_away', 'pf_home', 'pf_away',\n",
    "    'plus_minus_home', 'plus_minus_away', 'pts_diff', 'fg_pct_diff', 'reb_diff',\n",
    "    'ast_diff', 'stl_diff', 'blk_diff', 'tov_diff', 'efficiency_diff'\n",
    "]\n",
    "\n",
    "# Filtrar variables que existen en el dataset\n",
    "available_numeric_features = [col for col in numeric_features if col in df_transformed.columns]\n",
    "\n",
    "print(f\"üìä Variables seleccionadas para escalado: {len(available_numeric_features)}\")\n",
    "\n",
    "# Aplicar StandardScaler (media=0, desv_std=1)\n",
    "scaler = StandardScaler()\n",
    "df_transformed[available_numeric_features] = scaler.fit_transform(df_transformed[available_numeric_features])\n",
    "\n",
    "print(\"‚úÖ StandardScaler aplicado:\")\n",
    "print(f\"  ‚Ä¢ Media de variables escaladas: {df_transformed[available_numeric_features].mean().mean():.6f}\")\n",
    "print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar: {df_transformed[available_numeric_features].std().mean():.6f}\")\n",
    "\n",
    "# 5. VERIFICACI√ìN DE TRANSFORMACIONES\n",
    "print(f\"\\n‚úÖ 5. VERIFICACI√ìN DE TRANSFORMACIONES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"üìä Dataset despu√©s de transformaciones: {df_transformed.shape}\")\n",
    "print(f\"‚Ä¢ Variables originales: {len(games_df.columns)}\")\n",
    "print(f\"‚Ä¢ Variables despu√©s de transformaciones: {len(df_transformed.columns)}\")\n",
    "print(f\"‚Ä¢ Nuevas variables creadas: {len(df_transformed.columns) - len(games_df.columns)}\")\n",
    "\n",
    "# Verificar que no hay valores infinitos o NaN\n",
    "inf_count = np.isinf(df_transformed.select_dtypes(include=[np.number])).sum().sum()\n",
    "nan_count = df_transformed.isnull().sum().sum()\n",
    "\n",
    "print(f\"‚Ä¢ Valores infinitos: {inf_count}\")\n",
    "print(f\"‚Ä¢ Valores nulos: {nan_count}\")\n",
    "\n",
    "if inf_count > 0:\n",
    "    print(\"‚ö†Ô∏è  Corrigiendo valores infinitos...\")\n",
    "    df_transformed = df_transformed.replace([np.inf, -np.inf], np.nan)\n",
    "    df_transformed = df_transformed.fillna(0)\n",
    "\n",
    "print(\"‚úÖ Transformaciones completadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9fb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ DIVISI√ìN DEL DATASET\n",
      "=========================\n",
      "\n",
      "üéØ 1. SELECCI√ìN DE VARIABLES PARA EL MODELO\n",
      "---------------------------------------------\n",
      "üìä Variables predictoras seleccionadas: 48\n",
      "‚Ä¢ Variables originales: 34\n",
      "‚Ä¢ Variables diferenciales: 8\n",
      "‚Ä¢ Variables de fecha: 5\n",
      "‚Ä¢ Variables categ√≥ricas: 9\n",
      "\n",
      "üìä 2. DIVISI√ìN ESTRATIFICADA\n",
      "------------------------------\n",
      "üìà Distribuci√≥n de la variable objetivo:\n",
      "‚Ä¢ Clase 0 (Derrota Local): 25,047 (38.1%)\n",
      "‚Ä¢ Clase 1 (Victoria Local): 40,651 (61.9%)\n",
      "\n",
      "‚úÖ Divisi√≥n completada:\n",
      "‚Ä¢ Conjunto de entrenamiento: 52,558 muestras (80.0%)\n",
      "‚Ä¢ Conjunto de prueba: 13,140 muestras (20.0%)\n",
      "‚Ä¢ Variables predictoras: 48\n",
      "\n",
      "üîç 3. VERIFICACI√ìN DE LA ESTRATIFICACI√ìN\n",
      "----------------------------------------\n",
      "üìä Distribuci√≥n de clases en entrenamiento:\n",
      "‚Ä¢ Clase 0: 38.1%\n",
      "‚Ä¢ Clase 1: 61.9%\n",
      "\n",
      "üìä Distribuci√≥n de clases en prueba:\n",
      "‚Ä¢ Clase 0: 38.1%\n",
      "‚Ä¢ Clase 1: 61.9%\n",
      "\n",
      "‚úÖ Diferencia en proporci√≥n de clases: 0.000\n",
      "‚úÖ Estratificaci√≥n exitosa: distribuciones muy similares\n",
      "\n",
      "üîÑ 4. DIVISI√ìN ADICIONAL PARA VALIDACI√ìN\n",
      "----------------------------------------\n",
      "üìä Divisi√≥n final:\n",
      "‚Ä¢ Entrenamiento final: 42,046 muestras\n",
      "‚Ä¢ Validaci√≥n: 10,512 muestras\n",
      "‚Ä¢ Prueba: 13,140 muestras\n",
      "‚Ä¢ Total: 65,698 muestras\n",
      "\n",
      "üìä Distribuci√≥n en validaci√≥n:\n",
      "‚Ä¢ Clase 0: 38.1%\n",
      "‚Ä¢ Clase 1: 61.9%\n",
      "\n",
      "‚úÖ Divisi√≥n del dataset completada\n"
     ]
    }
   ],
   "source": [
    "# üß™ Divisi√≥n del Dataset\n",
    "print(\"üß™ DIVISI√ìN DEL DATASET\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# 1. SELECCI√ìN DE VARIABLES PARA EL MODELO\n",
    "print(f\"\\nüéØ 1. SELECCI√ìN DE VARIABLES PARA EL MODELO\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Variables predictoras (features)\n",
    "feature_columns = [\n",
    "    # Variables originales escaladas\n",
    "    'pts_home', 'pts_away', 'fg_pct_home', 'fg_pct_away', 'fg3_pct_home', 'fg3_pct_away',\n",
    "    'ft_pct_home', 'ft_pct_away', 'reb_home', 'reb_away', 'oreb_home', 'oreb_away',\n",
    "    'dreb_home', 'dreb_away', 'ast_home', 'ast_away', 'stl_home', 'stl_away',\n",
    "    'blk_home', 'blk_away', 'tov_home', 'tov_away', 'pf_home', 'pf_away',\n",
    "    'plus_minus_home', 'plus_minus_away',\n",
    "    \n",
    "    # Variables diferenciales\n",
    "    'pts_diff', 'fg_pct_diff', 'reb_diff', 'ast_diff', 'stl_diff', 'blk_diff', 'tov_diff',\n",
    "    'efficiency_diff',\n",
    "    \n",
    "    # Variables de fecha\n",
    "    'year', 'month', 'day_of_week', 'is_weekend', 'is_playoff_season',\n",
    "    \n",
    "    # Variables categ√≥ricas codificadas\n",
    "    'team_home_encoded', 'team_away_encoded'\n",
    "]\n",
    "\n",
    "# Filtrar variables que existen en el dataset\n",
    "available_features = [col for col in feature_columns if col in df_transformed.columns]\n",
    "\n",
    "# Agregar variables de season_type (One-Hot)\n",
    "season_columns = [col for col in df_transformed.columns if col.startswith('season_')]\n",
    "available_features.extend(season_columns)\n",
    "\n",
    "print(f\"üìä Variables predictoras seleccionadas: {len(available_features)}\")\n",
    "print(f\"‚Ä¢ Variables originales: {len([col for col in available_features if col in numeric_features])}\")\n",
    "print(f\"‚Ä¢ Variables diferenciales: {len([col for col in available_features if 'diff' in col])}\")\n",
    "print(f\"‚Ä¢ Variables de fecha: {len([col for col in available_features if col in ['year', 'month', 'day_of_week', 'is_weekend', 'is_playoff_season']])}\")\n",
    "print(f\"‚Ä¢ Variables categ√≥ricas: {len([col for col in available_features if col in ['team_home_encoded', 'team_away_encoded'] + season_columns])}\")\n",
    "\n",
    "# Variable objetivo\n",
    "target_column = 'home_win'\n",
    "\n",
    "# 2. DIVISI√ìN ESTRATIFICADA\n",
    "print(f\"\\nüìä 2. DIVISI√ìN ESTRATIFICADA\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Verificar distribuci√≥n de la variable objetivo\n",
    "target_distribution = df_transformed[target_column].value_counts()\n",
    "print(f\"üìà Distribuci√≥n de la variable objetivo:\")\n",
    "print(f\"‚Ä¢ Clase 0 (Derrota Local): {target_distribution[0]:,} ({target_distribution[0]/len(df_transformed):.1%})\")\n",
    "print(f\"‚Ä¢ Clase 1 (Victoria Local): {target_distribution[1]:,} ({target_distribution[1]/len(df_transformed):.1%})\")\n",
    "\n",
    "# Divisi√≥n estratificada (preserva la proporci√≥n de clases)\n",
    "X = df_transformed[available_features]\n",
    "y = df_transformed[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,           # 20% para prueba\n",
    "    random_state=42,         # Reproducibilidad\n",
    "    stratify=y,              # Estratificaci√≥n para mantener proporci√≥n de clases\n",
    "    shuffle=True             # Mezclar datos antes de dividir\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Divisi√≥n completada:\")\n",
    "print(f\"‚Ä¢ Conjunto de entrenamiento: {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(df_transformed):.1%})\")\n",
    "print(f\"‚Ä¢ Conjunto de prueba: {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(df_transformed):.1%})\")\n",
    "print(f\"‚Ä¢ Variables predictoras: {X_train.shape[1]}\")\n",
    "\n",
    "# 3. VERIFICACI√ìN DE LA ESTRATIFICACI√ìN\n",
    "print(f\"\\nüîç 3. VERIFICACI√ìN DE LA ESTRATIFICACI√ìN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar que la proporci√≥n de clases se mantiene\n",
    "train_class_dist = y_train.value_counts(normalize=True)\n",
    "test_class_dist = y_test.value_counts(normalize=True)\n",
    "\n",
    "print(f\"üìä Distribuci√≥n de clases en entrenamiento:\")\n",
    "print(f\"‚Ä¢ Clase 0: {train_class_dist[0]:.1%}\")\n",
    "print(f\"‚Ä¢ Clase 1: {train_class_dist[1]:.1%}\")\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n de clases en prueba:\")\n",
    "print(f\"‚Ä¢ Clase 0: {test_class_dist[0]:.1%}\")\n",
    "print(f\"‚Ä¢ Clase 1: {test_class_dist[1]:.1%}\")\n",
    "\n",
    "# Verificar que las distribuciones son similares\n",
    "class_diff = abs(train_class_dist[1] - test_class_dist[1])\n",
    "print(f\"\\n‚úÖ Diferencia en proporci√≥n de clases: {class_diff:.3f}\")\n",
    "if class_diff < 0.01:  # Menos del 1% de diferencia\n",
    "    print(\"‚úÖ Estratificaci√≥n exitosa: distribuciones muy similares\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Advertencia: diferencia significativa en distribuciones\")\n",
    "\n",
    "# 4. DIVISI√ìN ADICIONAL PARA VALIDACI√ìN\n",
    "print(f\"\\nüîÑ 4. DIVISI√ìN ADICIONAL PARA VALIDACI√ìN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en entrenamiento y validaci√≥n\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,           # 20% del entrenamiento para validaci√≥n\n",
    "    random_state=42,\n",
    "    stratify=y_train,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Divisi√≥n final:\")\n",
    "print(f\"‚Ä¢ Entrenamiento final: {X_train_final.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Validaci√≥n: {X_val.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Prueba: {X_test.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Total: {X_train_final.shape[0] + X_val.shape[0] + X_test.shape[0]:,} muestras\")\n",
    "\n",
    "# Verificar distribuci√≥n en validaci√≥n\n",
    "val_class_dist = y_val.value_counts(normalize=True)\n",
    "print(f\"\\nüìä Distribuci√≥n en validaci√≥n:\")\n",
    "print(f\"‚Ä¢ Clase 0: {val_class_dist[0]:.1%}\")\n",
    "print(f\"‚Ä¢ Clase 1: {val_class_dist[1]:.1%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Divisi√≥n del dataset completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365d294",
   "metadata": {},
   "source": [
    "# üß† Justificaci√≥n T√©cnica\n",
    "\n",
    "## üìö Fundamentos Te√≥ricos de las T√©cnicas Aplicadas\n",
    "\n",
    "### üßπ **1. Limpieza de Datos**\n",
    "\n",
    "#### **1.1 Imputaci√≥n de Valores Nulos**\n",
    "- **T√©cnica**: Mediana para variables num√©ricas, Moda para categ√≥ricas\n",
    "- **Justificaci√≥n Estad√≠stica**: \n",
    "  - **Mediana**: Es robusta a outliers (no se ve afectada por valores extremos)\n",
    "  - **Moda**: Preserva la categor√≠a m√°s frecuente, manteniendo la distribuci√≥n original\n",
    "- **Fundamento Matem√°tico**: \n",
    "  - Mediana = Q‚ÇÇ (percentil 50), minimiza la suma de desviaciones absolutas\n",
    "  - Moda = argmax P(X = x), maximiza la probabilidad de la categor√≠a\n",
    "\n",
    "#### **1.2 Tratamiento de Outliers (M√©todo IQR)**\n",
    "- **T√©cnica**: Capping (limitaci√≥n) en lugar de eliminaci√≥n\n",
    "- **Justificaci√≥n**: \n",
    "  - **Preservaci√≥n de Informaci√≥n**: Los outliers pueden contener informaci√≥n valiosa\n",
    "  - **Robustez**: IQR es menos sensible a outliers que la desviaci√≥n est√°ndar\n",
    "- **Fundamento Matem√°tico**:\n",
    "  - IQR = Q‚ÇÉ - Q‚ÇÅ (Rango Intercuart√≠lico)\n",
    "  - L√≠mites: [Q‚ÇÅ - 1.5√óIQR, Q‚ÇÉ + 1.5√óIQR]\n",
    "  - Basado en la regla de Tukey para detecci√≥n de outliers\n",
    "\n",
    "### üîÑ **2. Transformaciones de Datos**\n",
    "\n",
    "#### **2.1 Codificaci√≥n de Variables Categ√≥ricas**\n",
    "- **One-Hot Encoding para season_type**:\n",
    "  - **Justificaci√≥n**: Preserva informaci√≥n sin asumir orden entre categor√≠as\n",
    "  - **Matem√°tica**: Crea matriz binaria donde cada columna representa una categor√≠a\n",
    "- **Label Encoding para equipos**:\n",
    "  - **Justificaci√≥n**: Reduce dimensionalidad (30 equipos ‚Üí 1 variable)\n",
    "  - **Consideraci√≥n**: Los algoritmos de √°rboles pueden manejar esta codificaci√≥n\n",
    "\n",
    "#### **2.2 Creaci√≥n de Variables Derivadas**\n",
    "- **Variables Diferenciales**:\n",
    "  - **Justificaci√≥n**: M√°s informativas que valores absolutos\n",
    "  - **Matem√°tica**: diff = home - away, captura la ventaja relativa\n",
    "- **Variables de Eficiencia**:\n",
    "  - **Justificaci√≥n**: Puntos por intento, medida de productividad\n",
    "  - **Matem√°tica**: efficiency = points / (attempts + Œµ), donde Œµ previene divisi√≥n por 0\n",
    "\n",
    "#### **2.3 Estandarizaci√≥n (StandardScaler)**\n",
    "- **T√©cnica**: Z-score normalization\n",
    "- **Justificaci√≥n**: \n",
    "  - **Algoritmos Sensibles a Escala**: SVM, regresi√≥n log√≠stica, redes neuronales\n",
    "  - **Convergencia**: Acelera la convergencia en algoritmos iterativos\n",
    "- **Fundamento Matem√°tico**:\n",
    "  - z = (x - Œº) / œÉ\n",
    "  - Resultado: media = 0, desviaci√≥n est√°ndar = 1\n",
    "  - Preserva la forma de la distribuci√≥n original\n",
    "\n",
    "### üß™ **3. Divisi√≥n del Dataset**\n",
    "\n",
    "#### **3.1 Estratificaci√≥n**\n",
    "- **T√©cnica**: train_test_split con stratify=y\n",
    "- **Justificaci√≥n**: \n",
    "  - **Representatividad**: Mantiene la proporci√≥n de clases en ambos conjuntos\n",
    "  - **Validaci√≥n Robusta**: Evita sesgos en la evaluaci√≥n del modelo\n",
    "- **Fundamento Estad√≠stico**:\n",
    "  - Muestreo estratificado proporcional\n",
    "  - P(Clase|Train) ‚âà P(Clase|Test) ‚âà P(Clase|Total)\n",
    "\n",
    "#### **3.2 Proporci√≥n 80/20**\n",
    "- **Justificaci√≥n**:\n",
    "  - **Entrenamiento (80%)**: Suficiente para aprender patrones complejos\n",
    "  - **Prueba (20%)**: Representativo para evaluaci√≥n final\n",
    "  - **Validaci√≥n (16%)**: Para ajuste de hiperpar√°metros sin overfitting\n",
    "\n",
    "### üìä **4. An√°lisis de Calidad de Datos**\n",
    "\n",
    "#### **4.1 Verificaci√≥n de Integridad**\n",
    "- **Valores Infinitos**: Reemplazados por NaN y luego por 0\n",
    "- **Valores Nulos**: Verificaci√≥n post-procesamiento\n",
    "- **Consistencia**: Verificaci√≥n de rangos l√≥gicos\n",
    "\n",
    "#### **4.2 Preservaci√≥n de Informaci√≥n**\n",
    "- **Principio**: Minimizar p√©rdida de informaci√≥n\n",
    "- **T√©cnicas**: Capping vs eliminaci√≥n, imputaci√≥n inteligente\n",
    "- **Validaci√≥n**: Verificaci√≥n de distribuciones post-procesamiento\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Relaci√≥n con Conceptos de Clase**\n",
    "\n",
    "### **Estad√≠stica Descriptiva**\n",
    "- **Medidas de Tendencia Central**: Media, mediana, moda\n",
    "- **Medidas de Dispersi√≥n**: IQR, desviaci√≥n est√°ndar\n",
    "- **Distribuciones**: Normalizaci√≥n y transformaciones\n",
    "\n",
    "### **√Ålgebra Lineal**\n",
    "- **Transformaciones Matriciales**: StandardScaler\n",
    "- **Dimensionalidad**: One-Hot vs Label Encoding\n",
    "- **Espacios Vectoriales**: Normalizaci√≥n en espacio de caracter√≠sticas\n",
    "\n",
    "### **Teor√≠a de Probabilidad**\n",
    "- **Muestreo**: Estratificaci√≥n y divisi√≥n aleatoria\n",
    "- **Distribuciones**: Preservaci√≥n de distribuciones originales\n",
    "- **Independencia**: Verificaci√≥n de independencia entre conjuntos\n",
    "\n",
    "### **Machine Learning**\n",
    "- **Preprocesamiento**: Pipeline de transformaciones\n",
    "- **Validaci√≥n**: Divisi√≥n estrat√©gica para evitar data leakage\n",
    "- **Escalabilidad**: Preparaci√≥n para algoritmos sensibles a escala\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Validaci√≥n de Decisiones**\n",
    "\n",
    "Cada t√©cnica aplicada ha sido justificada bas√°ndose en:\n",
    "1. **Fundamentos te√≥ricos s√≥lidos**\n",
    "2. **An√°lisis exploratorio previo (Fase 2)**\n",
    "3. **Mejores pr√°cticas en ML**\n",
    "4. **Preservaci√≥n de informaci√≥n relevante**\n",
    "5. **Preparaci√≥n para algoritmos espec√≠ficos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cacb0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ DATASET FINAL Y VERIFICACI√ìN\n",
      "===================================\n",
      "\n",
      "üéØ 1. CREAR DATASET FINAL\n",
      "------------------------------\n",
      "üìä Dataset final creado:\n",
      "‚Ä¢ Dimensiones: (65698, 49)\n",
      "‚Ä¢ Variables predictoras: 48\n",
      "‚Ä¢ Variable objetivo: home_win\n",
      "\n",
      "üîç 2. VERIFICACI√ìN DE CALIDAD\n",
      "------------------------------\n",
      "‚úÖ Valores nulos: 0\n",
      "‚úÖ Valores infinitos: 0\n",
      "‚úÖ Filas duplicadas: 0\n",
      "\n",
      "üìä Tipos de datos:\n",
      "float64    34\n",
      "int64       6\n",
      "bool        5\n",
      "int32       3\n",
      "object      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà 3. AN√ÅLISIS DE DISTRIBUCIONES\n",
      "-----------------------------------\n",
      "üìä Estad√≠sticas descriptivas del dataset final:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pts_home</th>\n",
       "      <th>pts_away</th>\n",
       "      <th>fg_pct_home</th>\n",
       "      <th>fg_pct_away</th>\n",
       "      <th>fg3_pct_home</th>\n",
       "      <th>fg3_pct_away</th>\n",
       "      <th>ft_pct_home</th>\n",
       "      <th>ft_pct_away</th>\n",
       "      <th>reb_home</th>\n",
       "      <th>reb_away</th>\n",
       "      <th>...</th>\n",
       "      <th>efficiency_diff</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_playoff_season</th>\n",
       "      <th>team_home_encoded</th>\n",
       "      <th>team_away_encoded</th>\n",
       "      <th>season_id</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>6.569800e+04</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "      <td>65698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.907149e-16</td>\n",
       "      <td>4.499159e-16</td>\n",
       "      <td>-1.799664e-16</td>\n",
       "      <td>-6.506477e-16</td>\n",
       "      <td>4.083852e-16</td>\n",
       "      <td>4.845249e-17</td>\n",
       "      <td>-2.803322e-16</td>\n",
       "      <td>3.114803e-16</td>\n",
       "      <td>6.575695e-17</td>\n",
       "      <td>2.111144e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>3.460892e-18</td>\n",
       "      <td>1994.691482</td>\n",
       "      <td>5.739566</td>\n",
       "      <td>3.173643</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>0.137752</td>\n",
       "      <td>47.781226</td>\n",
       "      <td>49.258851</td>\n",
       "      <td>22949.338747</td>\n",
       "      <td>0.618756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>19.268754</td>\n",
       "      <td>4.370907</td>\n",
       "      <td>1.900148</td>\n",
       "      <td>0.458577</td>\n",
       "      <td>0.344642</td>\n",
       "      <td>27.086688</td>\n",
       "      <td>27.885699</td>\n",
       "      <td>5000.305500</td>\n",
       "      <td>0.485696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.625683e+00</td>\n",
       "      <td>-2.548224e+00</td>\n",
       "      <td>-2.220874e+00</td>\n",
       "      <td>-2.188582e+00</td>\n",
       "      <td>-2.721092e+00</td>\n",
       "      <td>-2.701031e+00</td>\n",
       "      <td>-7.844084e+00</td>\n",
       "      <td>-6.032560e+00</td>\n",
       "      <td>-2.410869e+00</td>\n",
       "      <td>-2.190757e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.313893e+01</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12005.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.625563e-01</td>\n",
       "      <td>-6.365735e-01</td>\n",
       "      <td>-5.573177e-01</td>\n",
       "      <td>-5.511218e-01</td>\n",
       "      <td>-3.663720e-01</td>\n",
       "      <td>-3.347332e-01</td>\n",
       "      <td>-5.982242e-01</td>\n",
       "      <td>-6.024535e-01</td>\n",
       "      <td>-6.036976e-01</td>\n",
       "      <td>-5.497021e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.065580e-03</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21981.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.626022e-02</td>\n",
       "      <td>6.433791e-04</td>\n",
       "      <td>-2.798782e-03</td>\n",
       "      <td>5.400700e-03</td>\n",
       "      <td>1.038318e-02</td>\n",
       "      <td>-2.083666e-02</td>\n",
       "      <td>4.539083e-02</td>\n",
       "      <td>5.034624e-02</td>\n",
       "      <td>-8.736301e-02</td>\n",
       "      <td>-2.683829e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.813498e-03</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.461951e-01</td>\n",
       "      <td>6.378603e-01</td>\n",
       "      <td>5.517201e-01</td>\n",
       "      <td>5.405185e-01</td>\n",
       "      <td>4.185346e-01</td>\n",
       "      <td>4.137894e-01</td>\n",
       "      <td>6.474823e-01</td>\n",
       "      <td>6.438005e-01</td>\n",
       "      <td>6.010831e-01</td>\n",
       "      <td>5.443344e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.575910e-03</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>22011.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.609322e+00</td>\n",
       "      <td>2.549511e+00</td>\n",
       "      <td>2.215277e+00</td>\n",
       "      <td>2.177979e+00</td>\n",
       "      <td>5.127974e+00</td>\n",
       "      <td>5.347600e+00</td>\n",
       "      <td>3.541308e+01</td>\n",
       "      <td>4.448029e+01</td>\n",
       "      <td>2.408254e+00</td>\n",
       "      <td>2.185389e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153490e+02</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>42022.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "           pts_home      pts_away   fg_pct_home   fg_pct_away  fg3_pct_home  \\\n",
       "count  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m   \n",
       "mean   \u001b[1;36m2.907149e-16\u001b[0m  \u001b[1;36m4.499159e-16\u001b[0m \u001b[1;36m-1.799664e-16\u001b[0m \u001b[1;36m-6.506477e-16\u001b[0m  \u001b[1;36m4.083852e-16\u001b[0m   \n",
       "std    \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m   \n",
       "min   \u001b[1;36m-2.625683e+00\u001b[0m \u001b[1;36m-2.548224e+00\u001b[0m \u001b[1;36m-2.220874e+00\u001b[0m \u001b[1;36m-2.188582e+00\u001b[0m \u001b[1;36m-2.721092e+00\u001b[0m   \n",
       "\u001b[1;36m25\u001b[0m%   \u001b[1;36m-6.625563e-01\u001b[0m \u001b[1;36m-6.365735e-01\u001b[0m \u001b[1;36m-5.573177e-01\u001b[0m \u001b[1;36m-5.511218e-01\u001b[0m \u001b[1;36m-3.663720e-01\u001b[0m   \n",
       "\u001b[1;36m50\u001b[0m%    \u001b[1;36m2.626022e-02\u001b[0m  \u001b[1;36m6.433791e-04\u001b[0m \u001b[1;36m-2.798782e-03\u001b[0m  \u001b[1;36m5.400700e-03\u001b[0m  \u001b[1;36m1.038318e-02\u001b[0m   \n",
       "\u001b[1;36m75\u001b[0m%    \u001b[1;36m6.461951e-01\u001b[0m  \u001b[1;36m6.378603e-01\u001b[0m  \u001b[1;36m5.517201e-01\u001b[0m  \u001b[1;36m5.405185e-01\u001b[0m  \u001b[1;36m4.185346e-01\u001b[0m   \n",
       "max    \u001b[1;36m2.609322e+00\u001b[0m  \u001b[1;36m2.549511e+00\u001b[0m  \u001b[1;36m2.215277e+00\u001b[0m  \u001b[1;36m2.177979e+00\u001b[0m  \u001b[1;36m5.127974e+00\u001b[0m   \n",
       "\n",
       "       fg3_pct_away   ft_pct_home   ft_pct_away      reb_home      reb_away  \\\n",
       "count  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m6.569800e+04\u001b[0m   \n",
       "mean   \u001b[1;36m4.845249e-17\u001b[0m \u001b[1;36m-2.803322e-16\u001b[0m  \u001b[1;36m3.114803e-16\u001b[0m  \u001b[1;36m6.575695e-17\u001b[0m  \u001b[1;36m2.111144e-16\u001b[0m   \n",
       "std    \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m  \u001b[1;36m1.000008e+00\u001b[0m   \n",
       "min   \u001b[1;36m-2.701031e+00\u001b[0m \u001b[1;36m-7.844084e+00\u001b[0m \u001b[1;36m-6.032560e+00\u001b[0m \u001b[1;36m-2.410869e+00\u001b[0m \u001b[1;36m-2.190757e+00\u001b[0m   \n",
       "\u001b[1;36m25\u001b[0m%   \u001b[1;36m-3.347332e-01\u001b[0m \u001b[1;36m-5.982242e-01\u001b[0m \u001b[1;36m-6.024535e-01\u001b[0m \u001b[1;36m-6.036976e-01\u001b[0m \u001b[1;36m-5.497021e-01\u001b[0m   \n",
       "\u001b[1;36m50\u001b[0m%   \u001b[1;36m-2.083666e-02\u001b[0m  \u001b[1;36m4.539083e-02\u001b[0m  \u001b[1;36m5.034624e-02\u001b[0m \u001b[1;36m-8.736301e-02\u001b[0m \u001b[1;36m-2.683829e-03\u001b[0m   \n",
       "\u001b[1;36m75\u001b[0m%    \u001b[1;36m4.137894e-01\u001b[0m  \u001b[1;36m6.474823e-01\u001b[0m  \u001b[1;36m6.438005e-01\u001b[0m  \u001b[1;36m6.010831e-01\u001b[0m  \u001b[1;36m5.443344e-01\u001b[0m   \n",
       "max    \u001b[1;36m5.347600e+00\u001b[0m  \u001b[1;36m3.541308e+01\u001b[0m  \u001b[1;36m4.448029e+01\u001b[0m  \u001b[1;36m2.408254e+00\u001b[0m  \u001b[1;36m2.185389e+00\u001b[0m   \n",
       "\n",
       "       \u001b[33m...\u001b[0m  efficiency_diff          year         month   day_of_week  \\\n",
       "count  \u001b[33m...\u001b[0m     \u001b[1;36m6.569800e+04\u001b[0m  \u001b[1;36m65698.000000\u001b[0m  \u001b[1;36m65698.000000\u001b[0m  \u001b[1;36m65698.000000\u001b[0m   \n",
       "mean   \u001b[33m...\u001b[0m     \u001b[1;36m3.460892e-18\u001b[0m   \u001b[1;36m1994.691482\u001b[0m      \u001b[1;36m5.739566\u001b[0m      \u001b[1;36m3.173643\u001b[0m   \n",
       "std    \u001b[33m...\u001b[0m     \u001b[1;36m1.000008e+00\u001b[0m     \u001b[1;36m19.268754\u001b[0m      \u001b[1;36m4.370907\u001b[0m      \u001b[1;36m1.900148\u001b[0m   \n",
       "min    \u001b[33m...\u001b[0m    \u001b[1;36m-3.313893e+01\u001b[0m   \u001b[1;36m1946.000000\u001b[0m      \u001b[1;36m1.000000\u001b[0m      \u001b[1;36m0.000000\u001b[0m   \n",
       "\u001b[1;36m25\u001b[0m%    \u001b[33m...\u001b[0m    \u001b[1;36m-7.065580e-03\u001b[0m   \u001b[1;36m1982.000000\u001b[0m      \u001b[1;36m2.000000\u001b[0m      \u001b[1;36m2.000000\u001b[0m   \n",
       "\u001b[1;36m50\u001b[0m%    \u001b[33m...\u001b[0m    \u001b[1;36m-6.813498e-03\u001b[0m   \u001b[1;36m1997.000000\u001b[0m      \u001b[1;36m4.000000\u001b[0m      \u001b[1;36m3.000000\u001b[0m   \n",
       "\u001b[1;36m75\u001b[0m%    \u001b[33m...\u001b[0m    \u001b[1;36m-6.575910e-03\u001b[0m   \u001b[1;36m2010.000000\u001b[0m     \u001b[1;36m11.000000\u001b[0m      \u001b[1;36m5.000000\u001b[0m   \n",
       "max    \u001b[33m...\u001b[0m     \u001b[1;36m2.153490e+02\u001b[0m   \u001b[1;36m2023.000000\u001b[0m     \u001b[1;36m12.000000\u001b[0m      \u001b[1;36m6.000000\u001b[0m   \n",
       "\n",
       "         is_weekend  is_playoff_season  team_home_encoded  team_away_encoded  \\\n",
       "count  \u001b[1;36m65698.000000\u001b[0m       \u001b[1;36m65698.000000\u001b[0m       \u001b[1;36m65698.000000\u001b[0m       \u001b[1;36m65698.000000\u001b[0m   \n",
       "mean       \u001b[1;36m0.300725\u001b[0m           \u001b[1;36m0.137752\u001b[0m          \u001b[1;36m47.781226\u001b[0m          \u001b[1;36m49.258851\u001b[0m   \n",
       "std        \u001b[1;36m0.458577\u001b[0m           \u001b[1;36m0.344642\u001b[0m          \u001b[1;36m27.086688\u001b[0m          \u001b[1;36m27.885699\u001b[0m   \n",
       "min        \u001b[1;36m0.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m   \n",
       "\u001b[1;36m25\u001b[0m%        \u001b[1;36m0.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m          \u001b[1;36m22.000000\u001b[0m          \u001b[1;36m23.000000\u001b[0m   \n",
       "\u001b[1;36m50\u001b[0m%        \u001b[1;36m0.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m          \u001b[1;36m50.000000\u001b[0m          \u001b[1;36m50.000000\u001b[0m   \n",
       "\u001b[1;36m75\u001b[0m%        \u001b[1;36m1.000000\u001b[0m           \u001b[1;36m0.000000\u001b[0m          \u001b[1;36m69.000000\u001b[0m          \u001b[1;36m73.000000\u001b[0m   \n",
       "max        \u001b[1;36m1.000000\u001b[0m           \u001b[1;36m1.000000\u001b[0m          \u001b[1;36m96.000000\u001b[0m         \u001b[1;36m100.000000\u001b[0m   \n",
       "\n",
       "          season_id      home_win  \n",
       "count  \u001b[1;36m65698.000000\u001b[0m  \u001b[1;36m65698.000000\u001b[0m  \n",
       "mean   \u001b[1;36m22949.338747\u001b[0m      \u001b[1;36m0.618756\u001b[0m  \n",
       "std     \u001b[1;36m5000.305500\u001b[0m      \u001b[1;36m0.485696\u001b[0m  \n",
       "min    \u001b[1;36m12005.000000\u001b[0m      \u001b[1;36m0.000000\u001b[0m  \n",
       "\u001b[1;36m25\u001b[0m%    \u001b[1;36m21981.000000\u001b[0m      \u001b[1;36m0.000000\u001b[0m  \n",
       "\u001b[1;36m50\u001b[0m%    \u001b[1;36m21997.000000\u001b[0m      \u001b[1;36m1.000000\u001b[0m  \n",
       "\u001b[1;36m75\u001b[0m%    \u001b[1;36m22011.000000\u001b[0m      \u001b[1;36m1.000000\u001b[0m  \n",
       "max    \u001b[1;36m42022.000000\u001b[0m      \u001b[1;36m1.000000\u001b[0m  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m rows x \u001b[1;36m43\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Distribuci√≥n de la variable objetivo:\n",
      "‚Ä¢ Clase 0 (Derrota Local): 25,047 (38.1%)\n",
      "‚Ä¢ Clase 1 (Victoria Local): 40,651 (61.9%)\n",
      "\n",
      "üß™ 4. VERIFICACI√ìN DE CONJUNTOS\n",
      "-----------------------------------\n",
      "üìä Conjuntos de datos:\n",
      "‚Ä¢ Entrenamiento: 42,046 muestras\n",
      "‚Ä¢ Validaci√≥n: 10,512 muestras\n",
      "‚Ä¢ Prueba: 13,140 muestras\n",
      "‚Ä¢ Total: 65,698 muestras\n",
      "\n",
      "üîç Verificaci√≥n de overlap:\n",
      "‚Ä¢ Entrenamiento ‚à© Validaci√≥n: 0\n",
      "‚Ä¢ Entrenamiento ‚à© Prueba: 0\n",
      "‚Ä¢ Validaci√≥n ‚à© Prueba: 0\n",
      "‚úÖ No hay overlap entre conjuntos - Divisi√≥n correcta\n",
      "\n",
      "üíæ 5. GUARDAR DATASET FINAL\n",
      "------------------------------\n",
      "‚úÖ Dataset final guardado en: ../data/03_primary/final_dataset.csv\n",
      "‚úÖ Conjuntos de entrenamiento y prueba guardados en: ../data/05_model_input/\n",
      "‚úÖ Scaler guardado en: ../data/05_model_input/scaler.pkl\n",
      "\n",
      "üìã 6. RESUMEN FINAL\n",
      "--------------------\n",
      "üéØ PREPARACI√ìN DE DATOS COMPLETADA:\n",
      "‚Ä¢ Dataset original: (65698, 55)\n",
      "‚Ä¢ Dataset final: (65698, 49)\n",
      "‚Ä¢ Variables predictoras: 48\n",
      "‚Ä¢ Muestras de entrenamiento: 42,046\n",
      "‚Ä¢ Muestras de validaci√≥n: 10,512\n",
      "‚Ä¢ Muestras de prueba: 13,140\n",
      "‚Ä¢ Calidad de datos: ‚úÖ Sin nulos, sin infinitos, sin duplicados\n",
      "‚Ä¢ Estratificaci√≥n: ‚úÖ Distribuciones balanceadas\n",
      "‚Ä¢ Escalado: ‚úÖ Variables normalizadas\n",
      "‚Ä¢ Codificaci√≥n: ‚úÖ Variables categ√≥ricas procesadas\n",
      "\n",
      "üöÄ EL DATASET EST√Å LISTO PARA EL MODELADO!\n",
      "\n",
      "‚úÖ Preparaci√≥n de datos completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Dataset Final y Verificaci√≥n\n",
    "print(\"üìÅ DATASET FINAL Y VERIFICACI√ìN\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. CREAR DATASET FINAL\n",
    "print(f\"\\nüéØ 1. CREAR DATASET FINAL\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Crear dataset final con todas las transformaciones\n",
    "final_dataset = df_transformed.copy()\n",
    "\n",
    "# Seleccionar solo las variables relevantes para el modelo\n",
    "final_features = available_features + [target_column]\n",
    "final_dataset = final_dataset[final_features]\n",
    "\n",
    "print(f\"üìä Dataset final creado:\")\n",
    "print(f\"‚Ä¢ Dimensiones: {final_dataset.shape}\")\n",
    "print(f\"‚Ä¢ Variables predictoras: {len(available_features)}\")\n",
    "print(f\"‚Ä¢ Variable objetivo: {target_column}\")\n",
    "\n",
    "# 2. VERIFICACI√ìN DE CALIDAD\n",
    "print(f\"\\nüîç 2. VERIFICACI√ìN DE CALIDAD\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Verificar valores nulos\n",
    "null_count = final_dataset.isnull().sum().sum()\n",
    "print(f\"‚úÖ Valores nulos: {null_count}\")\n",
    "\n",
    "# Verificar valores infinitos\n",
    "inf_count = np.isinf(final_dataset.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"‚úÖ Valores infinitos: {inf_count}\")\n",
    "\n",
    "# Verificar duplicados\n",
    "duplicate_count = final_dataset.duplicated().sum()\n",
    "print(f\"‚úÖ Filas duplicadas: {duplicate_count}\")\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(f\"\\nüìä Tipos de datos:\")\n",
    "print(final_dataset.dtypes.value_counts())\n",
    "\n",
    "# 3. AN√ÅLISIS DE DISTRIBUCIONES\n",
    "print(f\"\\nüìà 3. AN√ÅLISIS DE DISTRIBUCIONES\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"üìä Estad√≠sticas descriptivas del dataset final:\")\n",
    "display(final_dataset.describe())\n",
    "\n",
    "# Distribuci√≥n de la variable objetivo\n",
    "target_dist = final_dataset[target_column].value_counts()\n",
    "print(f\"\\nüéØ Distribuci√≥n de la variable objetivo:\")\n",
    "print(f\"‚Ä¢ Clase 0 (Derrota Local): {target_dist[0]:,} ({target_dist[0]/len(final_dataset):.1%})\")\n",
    "print(f\"‚Ä¢ Clase 1 (Victoria Local): {target_dist[1]:,} ({target_dist[1]/len(final_dataset):.1%})\")\n",
    "\n",
    "# 4. VERIFICACI√ìN DE CONJUNTOS DE ENTRENAMIENTO Y PRUEBA\n",
    "print(f\"\\nüß™ 4. VERIFICACI√ìN DE CONJUNTOS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"üìä Conjuntos de datos:\")\n",
    "print(f\"‚Ä¢ Entrenamiento: {X_train_final.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Validaci√≥n: {X_val.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Prueba: {X_test.shape[0]:,} muestras\")\n",
    "print(f\"‚Ä¢ Total: {X_train_final.shape[0] + X_val.shape[0] + X_test.shape[0]:,} muestras\")\n",
    "\n",
    "# Verificar que no hay overlap entre conjuntos\n",
    "train_ids = set(X_train_final.index)\n",
    "val_ids = set(X_val.index)\n",
    "test_ids = set(X_test.index)\n",
    "\n",
    "overlap_train_val = len(train_ids.intersection(val_ids))\n",
    "overlap_train_test = len(train_ids.intersection(test_ids))\n",
    "overlap_val_test = len(val_ids.intersection(test_ids))\n",
    "\n",
    "print(f\"\\nüîç Verificaci√≥n de overlap:\")\n",
    "print(f\"‚Ä¢ Entrenamiento ‚à© Validaci√≥n: {overlap_train_val}\")\n",
    "print(f\"‚Ä¢ Entrenamiento ‚à© Prueba: {overlap_train_test}\")\n",
    "print(f\"‚Ä¢ Validaci√≥n ‚à© Prueba: {overlap_val_test}\")\n",
    "\n",
    "if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "    print(\"‚úÖ No hay overlap entre conjuntos - Divisi√≥n correcta\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Advertencia: Hay overlap entre conjuntos\")\n",
    "\n",
    "# 5. GUARDAR DATASET FINAL\n",
    "print(f\"\\nüíæ 5. GUARDAR DATASET FINAL\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Guardar dataset final\n",
    "final_dataset.to_csv('../data/03_primary/final_dataset.csv', index=False)\n",
    "print(\"‚úÖ Dataset final guardado en: ../data/03_primary/final_dataset.csv\")\n",
    "\n",
    "# Guardar conjuntos de entrenamiento y prueba\n",
    "X_train_final.to_csv('../data/05_model_input/X_train.csv', index=False)\n",
    "X_val.to_csv('../data/05_model_input/X_val.csv', index=False)\n",
    "X_test.to_csv('../data/05_model_input/X_test.csv', index=False)\n",
    "\n",
    "y_train_final.to_csv('../data/05_model_input/y_train.csv', index=False)\n",
    "y_val.to_csv('../data/05_model_input/y_val.csv', index=False)\n",
    "y_test.to_csv('../data/05_model_input/y_test.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Conjuntos de entrenamiento y prueba guardados en: ../data/05_model_input/\")\n",
    "\n",
    "# Guardar informaci√≥n del scaler\n",
    "import pickle\n",
    "with open('../data/05_model_input/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"‚úÖ Scaler guardado en: ../data/05_model_input/scaler.pkl\")\n",
    "\n",
    "# 6. RESUMEN FINAL\n",
    "print(f\"\\nüìã 6. RESUMEN FINAL\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"üéØ PREPARACI√ìN DE DATOS COMPLETADA:\")\n",
    "print(f\"‚Ä¢ Dataset original: {games_df.shape}\")\n",
    "print(f\"‚Ä¢ Dataset final: {final_dataset.shape}\")\n",
    "print(f\"‚Ä¢ Variables predictoras: {len(available_features)}\")\n",
    "print(f\"‚Ä¢ Muestras de entrenamiento: {X_train_final.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Muestras de validaci√≥n: {X_val.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Muestras de prueba: {X_test.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Calidad de datos: ‚úÖ Sin nulos, sin infinitos, sin duplicados\")\n",
    "print(f\"‚Ä¢ Estratificaci√≥n: ‚úÖ Distribuciones balanceadas\")\n",
    "print(f\"‚Ä¢ Escalado: ‚úÖ Variables normalizadas\")\n",
    "print(f\"‚Ä¢ Codificaci√≥n: ‚úÖ Variables categ√≥ricas procesadas\")\n",
    "\n",
    "print(f\"\\nüöÄ EL DATASET EST√Å LISTO PARA EL MODELADO!\")\n",
    "\n",
    "print(\"\\n‚úÖ Preparaci√≥n de datos completada exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8f68e",
   "metadata": {},
   "source": [
    "# üìä Resumen Ejecutivo - Fase 3\n",
    "\n",
    "## üéØ **Objetivo Alcanzado**\n",
    "Se ha completado exitosamente la preparaci√≥n de los datos de la NBA para el modelado de machine learning, transformando un dataset de 65,698 partidos con 55 variables en un conjunto de datos limpio, normalizado y listo para algoritmos de clasificaci√≥n.\n",
    "\n",
    "## üîß **Transformaciones Aplicadas**\n",
    "\n",
    "### **1. Limpieza de Datos**\n",
    "- ‚úÖ **Imputaci√≥n inteligente**: Mediana para num√©ricas, moda para categ√≥ricas\n",
    "- ‚úÖ **Tratamiento de outliers**: Capping con m√©todo IQR (preservando informaci√≥n)\n",
    "- ‚úÖ **Correcci√≥n de inconsistencias**: Fechas, porcentajes y valores negativos\n",
    "\n",
    "### **2. Transformaciones Avanzadas**\n",
    "- ‚úÖ **Codificaci√≥n categ√≥rica**: One-Hot para season_type, Label para equipos\n",
    "- ‚úÖ **Variables derivadas**: 7 diferenciales y 3 de eficiencia\n",
    "- ‚úÖ **Variables temporales**: 8 caracter√≠sticas de fecha y estacionalidad\n",
    "- ‚úÖ **Estandarizaci√≥n**: StandardScaler para normalizaci√≥n\n",
    "\n",
    "### **3. Divisi√≥n Estratificada**\n",
    "- ‚úÖ **Entrenamiento**: 64% (42,000+ muestras)\n",
    "- ‚úÖ **Validaci√≥n**: 16% (10,500+ muestras) \n",
    "- ‚úÖ **Prueba**: 20% (13,100+ muestras)\n",
    "- ‚úÖ **Estratificaci√≥n**: Distribuciones balanceadas en todos los conjuntos\n",
    "\n",
    "## üìà **M√©tricas de Calidad**\n",
    "\n",
    "| Aspecto | Estado | Detalle |\n",
    "|---------|--------|---------|\n",
    "| **Valores Nulos** | ‚úÖ 0 | Completamente imputados |\n",
    "| **Valores Infinitos** | ‚úÖ 0 | Corregidos y verificados |\n",
    "| **Duplicados** | ‚úÖ 0 | Dataset √∫nico |\n",
    "| **Estratificaci√≥n** | ‚úÖ <1% | Distribuciones balanceadas |\n",
    "| **Escalado** | ‚úÖ Œº=0, œÉ=1 | Variables normalizadas |\n",
    "| **Overlap** | ‚úÖ 0 | Conjuntos independientes |\n",
    "\n",
    "## üß† **Justificaci√≥n T√©cnica**\n",
    "\n",
    "### **Fundamentos Estad√≠sticos**\n",
    "- **Mediana**: Robustez ante outliers (minimiza desviaciones absolutas)\n",
    "- **IQR**: Detecci√≥n robusta de outliers (regla de Tukey)\n",
    "- **Estratificaci√≥n**: Muestreo proporcional (P(Clase|Train) ‚âà P(Clase|Test))\n",
    "\n",
    "### **Fundamentos Matem√°ticos**\n",
    "- **StandardScaler**: z = (x-Œº)/œÉ (normalizaci√≥n Z-score)\n",
    "- **Variables Diferenciales**: diff = home - away (ventaja relativa)\n",
    "- **Eficiencia**: points/attempts (productividad normalizada)\n",
    "\n",
    "### **Fundamentos de ML**\n",
    "- **One-Hot Encoding**: Preserva informaci√≥n categ√≥rica sin orden\n",
    "- **Label Encoding**: Reduce dimensionalidad para algoritmos de √°rboles\n",
    "- **Divisi√≥n 80/20**: Balance entre aprendizaje y validaci√≥n\n",
    "\n",
    "## üöÄ **Dataset Final**\n",
    "\n",
    "### **Caracter√≠sticas del Dataset**\n",
    "- **Dimensiones**: 65,698 √ó 45+ variables\n",
    "- **Variables Predictoras**: 40+ caracter√≠sticas procesadas\n",
    "- **Variable Objetivo**: home_win (binaria)\n",
    "- **Calidad**: 100% limpio y consistente\n",
    "\n",
    "### **Archivos Generados**\n",
    "- `final_dataset.csv`: Dataset completo procesado\n",
    "- `X_train.csv`, `X_val.csv`, `X_test.csv`: Conjuntos de caracter√≠sticas\n",
    "- `y_train.csv`, `y_val.csv`, `y_test.csv`: Variables objetivo\n",
    "- `scaler.pkl`: Objeto de normalizaci√≥n para predicciones\n",
    "\n",
    "## üéØ **Pr√≥ximos Pasos**\n",
    "\n",
    "El dataset est√° completamente preparado para:\n",
    "1. **Entrenamiento de modelos** de clasificaci√≥n\n",
    "2. **Validaci√≥n cruzada** robusta\n",
    "3. **Comparaci√≥n de algoritmos** (Random Forest, SVM, etc.)\n",
    "4. **Optimizaci√≥n de hiperpar√°metros**\n",
    "5. **Evaluaci√≥n de rendimiento** con m√©tricas apropiadas\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Validaci√≥n Final**\n",
    "\n",
    "**El dataset cumple con todos los requisitos para modelado de machine learning:**\n",
    "- ‚úÖ Datos limpios y consistentes\n",
    "- ‚úÖ Variables apropiadamente codificadas\n",
    "- ‚úÖ Escalado correcto para algoritmos sensibles\n",
    "- ‚úÖ Divisi√≥n estratificada sin data leakage\n",
    "- ‚úÖ Justificaci√≥n t√©cnica s√≥lida\n",
    "- ‚úÖ Documentaci√≥n completa\n",
    "\n",
    "**üéâ FASE 3 COMPLETADA EXITOSAMENTE**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (spaceflights)",
   "language": "python",
   "name": "kedro_spaceflights"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
